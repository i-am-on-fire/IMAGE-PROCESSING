{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "987ff1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from io import BytesIO\n",
    "import tarfile\n",
    "import tempfile\n",
    "from six.moves import urllib\n",
    "\n",
    "from matplotlib import gridspec\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "#%tensorflow_version 1.x\n",
    "import tensorflow as tf\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "723c22d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.load(r'FeatureFiles/train_imagedata.npy')\n",
    "test_data = np.load(r'FeatureFiles/test_imagedata.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "382d0c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepLabModel(object):\n",
    "    \"\"\"Class to load deeplab model and run inference.\"\"\"\n",
    "    \n",
    "    INPUT_TENSOR_NAME = 'ImageTensor:0'\n",
    "    OUTPUT_TENSOR_NAME = 'SemanticPredictions:0'\n",
    "    INPUT_SIZE = 513\n",
    "    FROZEN_GRAPH_NAME = 'frozen_inference_graph'\n",
    "    \n",
    "    def __init__(self, tarball_path):\n",
    "        \"\"\"Creates and loads pretrained deeplab model.\"\"\"\n",
    "        self.graph = tf.Graph()\n",
    "\n",
    "        graph_def = None\n",
    "        # Extract frozen graph from tar archive.\n",
    "        tar_file = tarfile.open(tarball_path)\n",
    "        for tar_info in tar_file.getmembers():\n",
    "            if self.FROZEN_GRAPH_NAME in os.path.basename(tar_info.name):\n",
    "                file_handle = tar_file.extractfile(tar_info)\n",
    "                #graph_def = tf.GraphDef.FromString(file_handle.read())\n",
    "                #graph = tf.Graph()\n",
    "                graph_def = tf.compat.v1.GraphDef.FromString(file_handle.read())\n",
    "                break\n",
    "\n",
    "        tar_file.close()\n",
    "        \n",
    "        if graph_def is None:\n",
    "            raise RuntimeError('Cannot find inference graph in tar archive.')\n",
    "            \n",
    "        with self.graph.as_default():\n",
    "            tf.import_graph_def(graph_def, name='')\n",
    "\n",
    "        self.sess = tf.compat.v1.Session(graph=self.graph)\n",
    "        \n",
    "    def run(self, image):\n",
    "        \"\"\"Runs inference on a single image.\n",
    "\n",
    "    Args:\n",
    "      image: A PIL.Image object, raw input image.\n",
    "\n",
    "    Returns:\n",
    "      resized_image: RGB image resized from original input image.\n",
    "      seg_map: Segmentation map of `resized_image`.\n",
    "    \"\"\"\n",
    "        #width, height = image.shape[0], image.shape[1]\n",
    "        width, height = image.size\n",
    "        resize_ratio = 1.0 * self.INPUT_SIZE / max(width, height)\n",
    "        target_size = (int(resize_ratio * width), int(resize_ratio * height))\n",
    "        resized_image = image.convert('RGB').resize(target_size, Image.ANTIALIAS)\n",
    "        #resized_image = np.resize(image, target_size)\n",
    "        batch_seg_map = self.sess.run(\n",
    "            self.OUTPUT_TENSOR_NAME,\n",
    "            feed_dict={self.INPUT_TENSOR_NAME: [np.asarray(resized_image)]})\n",
    "        seg_map = batch_seg_map[0]\n",
    "        \n",
    "        return resized_image, seg_map\n",
    "    \n",
    "def create_ade20k_label_colormap():\n",
    "        \n",
    "    \"\"\"Creates a label colormap used in ADE20K segmentation benchmark.\n",
    "\n",
    "  Returns:\n",
    "    A colormap for visualizing segmentation results.\n",
    "  \"\"\"\n",
    "    colormap = np.asarray([\n",
    "        [0,0,0],\n",
    "        [120, 120, 120],\n",
    "        [180, 120, 120],\n",
    "        [6, 230, 230],\n",
    "        [80, 50, 50],\n",
    "        [4, 200, 3],\n",
    "        [120, 120, 80],\n",
    "        [140, 140, 140],\n",
    "        [204, 5, 255],\n",
    "        [230, 230, 230],\n",
    "        [4, 250, 7],\n",
    "        [224, 5, 255],\n",
    "        [235, 255, 7],\n",
    "        [150, 5, 61],\n",
    "        [120, 120, 70],\n",
    "        [8, 255, 51],\n",
    "        [255, 6, 82],\n",
    "        [143, 255, 140],\n",
    "        [204, 255, 4],\n",
    "        [255, 51, 7],\n",
    "        [204, 70, 3],\n",
    "        [0, 102, 200],\n",
    "        [61, 230, 250],\n",
    "        [255, 6, 51],\n",
    "        [11, 102, 255],\n",
    "        [255, 7, 71],\n",
    "        [255, 9, 224],\n",
    "        [9, 7, 230],\n",
    "        [220, 220, 220],\n",
    "        [255, 9, 92],\n",
    "        [112, 9, 255],\n",
    "        [8, 255, 214],\n",
    "        [7, 255, 224],\n",
    "        [255, 184, 6],\n",
    "        [10, 255, 71],\n",
    "        [255, 41, 10],\n",
    "        [7, 255, 255],\n",
    "        [224, 255, 8],\n",
    "        [102, 8, 255],\n",
    "        [255, 61, 6],\n",
    "        [255, 194, 7],\n",
    "        [255, 122, 8],\n",
    "        [0, 255, 20],\n",
    "        [255, 8, 41],\n",
    "        [255, 5, 153],\n",
    "        [6, 51, 255],\n",
    "        [235, 12, 255],\n",
    "        [160, 150, 20],\n",
    "        [0, 163, 255],\n",
    "        [140, 140, 140],\n",
    "        [250, 10, 15],\n",
    "        [20, 255, 0],\n",
    "        [31, 255, 0],\n",
    "        [255, 31, 0],\n",
    "        [255, 224, 0],\n",
    "        [153, 255, 0],\n",
    "        [0, 0, 255],\n",
    "        [255, 71, 0],\n",
    "        [0, 235, 255],\n",
    "        [0, 173, 255],\n",
    "        [31, 0, 255],\n",
    "        [11, 200, 200],\n",
    "        [255, 82, 0],\n",
    "        [0, 255, 245],\n",
    "        [0, 61, 255],\n",
    "        [0, 255, 112],\n",
    "        [0, 255, 133],\n",
    "        [255, 0, 0],\n",
    "        [255, 163, 0],\n",
    "        [255, 102, 0],\n",
    "        [194, 255, 0],\n",
    "        [0, 143, 255],\n",
    "        [51, 255, 0],\n",
    "        [0, 82, 255],\n",
    "        [0, 255, 41],\n",
    "        [0, 255, 173],\n",
    "        [10, 0, 255],\n",
    "        [173, 255, 0],\n",
    "        [0, 255, 153],\n",
    "        [255, 92, 0],\n",
    "        [255, 0, 255],\n",
    "        [255, 0, 245],\n",
    "        [255, 0, 102],\n",
    "        [255, 173, 0],\n",
    "        [255, 0, 20],\n",
    "        [255, 184, 184],\n",
    "        [0, 31, 255],\n",
    "        [0, 255, 61],\n",
    "        [0, 71, 255],\n",
    "        [255, 0, 204],\n",
    "        [0, 255, 194],\n",
    "        [0, 255, 82],\n",
    "        [0, 10, 255],\n",
    "        [0, 112, 255],\n",
    "        [51, 0, 255],\n",
    "        [0, 194, 255],\n",
    "        [0, 122, 255],\n",
    "        [0, 255, 163],\n",
    "        [255, 153, 0],\n",
    "        [0, 255, 10],\n",
    "        [255, 112, 0],\n",
    "        [143, 255, 0],\n",
    "        [82, 0, 255],\n",
    "        [163, 255, 0],\n",
    "        [255, 235, 0],\n",
    "        [8, 184, 170],\n",
    "        [133, 0, 255],\n",
    "        [0, 255, 92],\n",
    "        [184, 0, 255],\n",
    "        [255, 0, 31],\n",
    "        [0, 184, 255],\n",
    "        [0, 214, 255],\n",
    "        [255, 0, 112],\n",
    "        [92, 255, 0],\n",
    "        [0, 224, 255],\n",
    "        [112, 224, 255],\n",
    "        [70, 184, 160],\n",
    "        [163, 0, 255],\n",
    "        [153, 0, 255],\n",
    "        [71, 255, 0],\n",
    "        [255, 0, 163],\n",
    "        [255, 204, 0],\n",
    "        [255, 0, 143],\n",
    "        [0, 255, 235],\n",
    "        [133, 255, 0],\n",
    "        [255, 0, 235],\n",
    "        [245, 0, 255],\n",
    "        [255, 0, 122],\n",
    "        [255, 245, 0],\n",
    "        [10, 190, 212],\n",
    "        [214, 255, 0],\n",
    "        [0, 204, 255],\n",
    "        [20, 0, 255],\n",
    "        [255, 255, 0],\n",
    "        [0, 153, 255],\n",
    "        [0, 41, 255],\n",
    "        [0, 255, 204],\n",
    "        [41, 0, 255],\n",
    "        [41, 255, 0],\n",
    "        [173, 0, 255],\n",
    "        [0, 245, 255],\n",
    "        [71, 0, 255],\n",
    "        [122, 0, 255],\n",
    "        [0, 255, 184],\n",
    "        [0, 92, 255],\n",
    "        [184, 255, 0],\n",
    "        [0, 133, 255],\n",
    "        [255, 214, 0],\n",
    "        [25, 194, 194],\n",
    "        [102, 255, 0],\n",
    "        [92, 0, 255],\n",
    "    ])\n",
    "    return colormap\n",
    "\n",
    "def label_to_color_image(label):\n",
    "        \"\"\"Adds color defined by the dataset colormap to the label.\n",
    "\n",
    "  Args:\n",
    "    label: A 2D array with integer type, storing the segmentation label.\n",
    "\n",
    "  Returns:\n",
    "    result: A 2D array with floating type. The element of the array\n",
    "      is the color indexed by the corresponding element in the input label\n",
    "      to the ADE20K color map.\n",
    "\n",
    "  Raises:\n",
    "    ValueError: If label is not of rank 2 or its value is larger than color\n",
    "      map maximum entry.\n",
    "  \"\"\"\n",
    "        if label.ndim != 2:\n",
    "            raise ValueError('Expect 2-D input label')\n",
    "        colormap = create_ade20k_label_colormap()\n",
    "        \n",
    "        if np.max(label) >= len(colormap):\n",
    "            raise ValueError('label value too large.')\n",
    "            \n",
    "        return colormap[label]\n",
    "    \n",
    "def vis_segmentation(image, seg_map):\n",
    "    \"\"\"Visualizes input image, segmentation map and overlay view.\"\"\"\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    grid_spec = gridspec.GridSpec(1, 4, width_ratios=[6, 6, 6, 1]) #分为 1行4列,\n",
    "\n",
    "    plt.subplot(grid_spec[0])  # 4个图的第一部分\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    plt.title('input image')\n",
    "\n",
    "    plt.subplot(grid_spec[1])\n",
    "    seg_image = label_to_color_image(seg_map).astype(np.uint8)\n",
    "    plt.imshow(seg_image)\n",
    "    plt.axis('off')\n",
    "    plt.title('segmentation map')\n",
    "\n",
    "    plt.subplot(grid_spec[2])\n",
    "    plt.imshow(image)\n",
    "    plt.imshow(seg_image, alpha=0.7)\n",
    "    plt.axis('off')\n",
    "    plt.title('segmentation overlay')\n",
    "\n",
    "    unique_labels = np.unique(seg_map)\n",
    "        \n",
    "        \n",
    "    ax = plt.subplot(grid_spec[3])\n",
    "    plt.imshow(\n",
    "        FULL_COLOR_MAP[unique_labels].astype(np.uint8), interpolation='nearest')\n",
    "    ax.yaxis.tick_right()\n",
    "    plt.yticks(range(len(unique_labels)), LABEL_NAMES[unique_labels])\n",
    "    plt.xticks([], [])\n",
    "    ax.tick_params(width=0.0)\n",
    "    plt.grid('off')\n",
    "    plt.show()\n",
    "        \n",
    "    size = seg_image.shape[0] * seg_image.shape[1]\n",
    "    selLABEL_NAMES = np.asarray([\n",
    "                'tree', 'shrub', 'grass','road','sidewalk','sky','building'\n",
    "                ])\n",
    "    ratio_sum = ratio_plant = 0\n",
    "    ratio_tree = ratio_shrub = ratio_grass = ratio_road = ratio_sidewalk = 0\n",
    "        \n",
    "    for m in seg_map.flatten():\n",
    "        label = LABEL_NAMES[m]\n",
    "        if label == 'tree':\n",
    "            ratio_tree = ratio_tree+1\n",
    "        if label == 'shrub':\n",
    "            ratio_shrub = ratio_shrub+1\n",
    "        if label == 'grass':\n",
    "            ratio_grass = ratio_grass+1  \n",
    "        if label == 'road':\n",
    "            ratio_road = ratio_road+1 \n",
    "        if label == 'sidewalk':\n",
    "            ratio_sidewalk = ratio_sidewalk+1\n",
    "                \n",
    "        for n in selLABEL_NAMES:\n",
    "            if label == n:\n",
    "                ratio_sum = ratio_sum + 1\n",
    "            \n",
    "    ratio_tree_i = ratio_tree / len(seg_map.flatten())\n",
    "    ratio_shrub_i = ratio_shrub / len(seg_map.flatten())\n",
    "    ratio_grass_i = ratio_grass / len(seg_map.flatten())\n",
    "    ratio_road_i = ratio_road / len(seg_map.flatten()) \n",
    "    ratio_sidewalk_i = ratio_sidewalk / len(seg_map.flatten()) \n",
    "\n",
    "    ratio_plant_i = (ratio_tree + ratio_shrub + ratio_grass)/ len(seg_map.flatten())\n",
    "    #ratio_sum_i = ratio_sum / len(seg_map.flatten())\n",
    "    print('tree合计占比为：',ratio_tree_i)\n",
    "    print('shrub合计占比为：',ratio_shrub_i)\n",
    "    print('grass合计占比为：',ratio_grass_i)\n",
    "    print('road合计占比为：',ratio_road_i)\n",
    "    print('sidewalk合计占比为：',ratio_sidewalk_i)\n",
    "    print('plant经过计算植物合计占比为：',ratio_plant_i)\n",
    "        \n",
    "    return ratio_tree_i, ratio_shrub_i, ratio_grass_i, ratio_plant_i, ratio_road_i ,ratio_sidewalk_i\n",
    "\n",
    "LABEL_NAMES = np.asarray([\n",
    "        'ignore', 'wall', 'building', 'sky', 'floor', 'tree', 'ceiling', 'road', \n",
    "         'bed', 'window', 'grass', 'cabinet', 'sidewalk', 'person', 'ground', 'door', \n",
    "        'table', 'mount', 'shrub', 'curtain', 'chair', 'car', 'water', 'picture', \n",
    "        'couch', 'shelf', 'house', 'sea', 'mirror', 'rug', 'field', 'armchair',\n",
    "        'seat', 'fence', 'desk', 'rock', 'clothes', 'lamp', 'bath', 'rail', 'cushion',\n",
    "        'stand', 'box', 'pillar', 'signboard', 'drawers', 'counter', 'sand', 'sink',\n",
    "        'skyscraper', 'fireplace', 'refrigerator', 'cupboard', 'path', 'steps',\n",
    "        'runway', 'case', 'pool', 'pillow', 'screen', 'stairway', 'river', \n",
    "        'bridge', 'bookcase', 'blinds', 'coffeeTable', 'toilet', 'flower', 'book',\n",
    "        'hill', 'bench', 'countertop', 'kitchen Sove', 'tree', 'kitchen', \n",
    "        'computingMachine', 'chair', 'boat', 'bar', 'machine', 'hut', 'bus', \n",
    "        'towel', 'light', 'truck', 'tower', 'chandelier', 'awning', 'streetlight',\n",
    "        'booth', 'displayMonitor', 'airplane', 'dirtTrack', 'apparel', 'pole', \n",
    "        'ground', 'handrail', 'escalator', 'ottoman', 'bottle', 'counter', 'poster', \n",
    "        'stage', 'van', 'ship', 'fountain', 'conveyor', 'canopy', 'washer', 'toy', \n",
    "        'swimmingPool', 'stool', 'barrel', 'basket', 'waterfall', 'tent', 'bag', \n",
    "        'bike', 'cradle', 'oven', 'ball', 'food', 'step', 'container', 'brandLogo', \n",
    "        'oven', 'pot', 'animal', 'bicycle', 'lake', 'dishwasher', 'projectorScreen', \n",
    "        'blanket', 'statue', 'hood', 'sconce', 'vase', 'trafficLight', 'tray', \n",
    "        'GarbageBin', 'fan', 'dock', 'computerMonitor', 'plate', 'monitoringDevice', \n",
    "        'bulletinBoard', 'shower', 'radiator', 'drinkingGlass', 'clock', 'flag'\n",
    "     ])\n",
    "\n",
    "FULL_LABEL_MAP = np.arange(len(LABEL_NAMES)).reshape(len(LABEL_NAMES), 1)\n",
    "#print('FULL_LABEL_MAP',FULL_LABEL_MAP)\n",
    "\n",
    "FULL_COLOR_MAP = label_to_color_image(FULL_LABEL_MAP)\n",
    "#print('FULL_COLOR_MAP',FULL_COLOR_MAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9491b31e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading model, this might take a while...\n",
      "download completed! loading DeepLab model...\n",
      "model loaded successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-26 14:20:01.419418: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-26 14:20:01.419548: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-26 14:20:01.461961: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/stud1/.local/lib/python3.8/site-packages/cv2/../../lib64:\n",
      "2022-07-26 14:20:01.461972: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-07-26 14:20:01.462402: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = 'xception65_ade20k_train'  # @param ['mobilenetv2_ade20k_train', 'xception65_ade20k_train']\n",
    "\n",
    "_DOWNLOAD_URL_PREFIX = 'http://download.tensorflow.org/models/'\n",
    "_MODEL_URLS = {\n",
    "    'mobilenetv2_ade20k_train':\n",
    "        'deeplabv3_mnv2_ade20k_train_2018_12_03.tar.gz',\n",
    "    'xception65_ade20k_train':\n",
    "        'deeplabv3_xception_ade20k_train_2018_05_29.tar.gz',\n",
    "}\n",
    "_TARBALL_NAME = 'deeplab_model.tar.gz'\n",
    "\n",
    "model_dir = tempfile.mkdtemp()\n",
    "tf.io.gfile.makedirs(model_dir)\n",
    "\n",
    "download_path = os.path.join(model_dir, _TARBALL_NAME)\n",
    "print('downloading model, this might take a while...')\n",
    "urllib.request.urlretrieve(_DOWNLOAD_URL_PREFIX + _MODEL_URLS[MODEL_NAME],\n",
    "                   download_path)\n",
    "print('download completed! loading DeepLab model...')\n",
    "\n",
    "MODEL = DeepLabModel(download_path)\n",
    "MODEL.INPUT_SIZE = 420\n",
    "\n",
    "# Reduce image size if mobilenet model\n",
    "if \"mobilenetv2\" in MODEL_NAME:\n",
    "    MODEL.INPUT_SIZE = 257\n",
    "\n",
    "print('model loaded successfully!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a46d598",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started for train...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-26 14:20:09.753104: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:354] MLIR V1 optimization pass is not enabled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done for image 0\n",
      "Done for image 100\n",
      "Done for image 200\n",
      "Done for image 300\n",
      "Done for image 400\n",
      "Done for image 500\n",
      "Done for image 600\n",
      "Done for image 700\n",
      "Done for image 800\n",
      "Done for image 900\n",
      "Done for image 1000\n",
      "Done for image 1100\n",
      "Done for image 1200\n",
      "Done for image 1300\n",
      "Done for image 1400\n",
      "Done for image 1500\n",
      "Done for image 1600\n",
      "Done for image 1700\n",
      "Done for image 1800\n",
      "Done for image 1900\n",
      "Done for image 2000\n",
      "Done for image 2100\n",
      "Done for image 2200\n",
      "Done for image 2300\n",
      "Done for image 2400\n",
      "Done for image 2500\n",
      "Done for image 2600\n",
      "Done for image 2700\n",
      "Done for image 2800\n",
      "Done for image 2900\n",
      "Done for image 3000\n",
      "Done for image 3100\n",
      "Done for image 3200\n",
      "Done for image 3300\n",
      "Done for image 3400\n",
      "Done for image 3500\n",
      "Done for image 3600\n",
      "Done for image 3700\n",
      "Done for image 3800\n",
      "Done for image 3900\n",
      "Done for image 4000\n",
      "Done for image 4100\n",
      "Done for image 4200\n",
      "Done for image 4300\n",
      "Done for image 4400\n",
      "Done for image 4500\n",
      "Done for image 4600\n",
      "Done for image 4700\n",
      "Done for image 4800\n",
      "Done for image 4900\n",
      "Done for image 5000\n",
      "Done for image 5100\n",
      "Done for image 5200\n",
      "Done for image 5300\n",
      "Done for image 5400\n",
      "Done for image 5500\n",
      "Done for image 5600\n",
      "Done for image 5700\n",
      "Done for image 5800\n",
      "Done for image 5900\n",
      "Done for image 6000\n",
      "Done for image 6100\n",
      "Done for image 6200\n",
      "Done for image 6300\n",
      "Done for image 6400\n",
      "Done for image 6500\n",
      "Done for image 6600\n",
      "Done for image 6700\n",
      "Done for image 6800\n",
      "Done for image 6900\n",
      "Done for image 7000\n",
      "Done for image 7100\n",
      "Done for image 7200\n",
      "Done for image 7300\n",
      "Done for image 7400\n",
      "Done for image 7500\n",
      "Done for image 7600\n",
      "Done for image 7700\n",
      "Done for image 7800\n",
      "Done for image 7900\n",
      "Done for image 8000\n",
      "Done for image 8100\n",
      "Done for image 8200\n",
      "Done for image 8300\n",
      "Done for image 8400\n",
      "Done for image 8500\n",
      "Done for image 8600\n",
      "Done for image 8700\n",
      "Done for image 8800\n",
      "Done for image 8900\n",
      "Done for image 9000\n",
      "Done for image 9100\n",
      "Done for image 9200\n",
      "Done for image 9300\n",
      "Done for image 9400\n",
      "Done for image 9500\n",
      "Done for image 9600\n",
      "Done for image 9700\n",
      "Done for image 9800\n",
      "Done for image 9900\n",
      "Done for image 10000\n",
      "Done for image 10100\n",
      "Done for image 10200\n",
      "Done for image 10300\n",
      "Done for image 10400\n",
      "Done for image 10500\n",
      "Done for image 10600\n",
      "Done for image 10700\n",
      "Done for image 10800\n",
      "Done for image 10900\n",
      "Done for image 11000\n",
      "Done for image 11100\n",
      "Done for image 11200\n",
      "Done for image 11300\n",
      "Done for image 11400\n",
      "Done for image 11500\n",
      "Done for image 11600\n",
      "Done for image 11700\n",
      "Done for image 11800\n",
      "Done for image 11900\n",
      "Done for image 12000\n",
      "Done for image 12100\n",
      "Done for image 12200\n",
      "Done for image 12300\n",
      "Done for image 12400\n",
      "Done for image 12500\n",
      "Done for image 12600\n",
      "Done for image 12700\n",
      "Done for image 12800\n",
      "Done for image 12900\n",
      "Done for image 13000\n",
      "Done for image 13100\n",
      "Done for image 13200\n",
      "Done for image 13300\n",
      "Done for image 13400\n",
      "Done for image 13500\n",
      "Done for image 13600\n",
      "Done for image 13700\n",
      "Done for image 13800\n",
      "Done for image 13900\n",
      "Done for image 14000\n",
      "Done for image 14100\n",
      "Done for image 14200\n",
      "Done for image 14300\n",
      "Done for image 14400\n",
      "Done for image 14500\n",
      "Done for image 14600\n",
      "Done for image 14700\n",
      "Done for image 14800\n",
      "Done for image 14900\n",
      "Done for image 15000\n",
      "Done for image 15100\n",
      "Done for image 15200\n",
      "Done for image 15300\n",
      "Done for image 15400\n",
      "Done for image 15500\n",
      "Done for image 15600\n",
      "Done for image 15700\n",
      "Done for image 15800\n",
      "Done for image 15900\n",
      "Done for image 16000\n",
      "Done for image 16100\n",
      "Done for image 16200\n",
      "Done for image 16300\n",
      "Done for image 16400\n",
      "Done for image 16500\n",
      "Done for image 16600\n",
      "Done for image 16700\n",
      "Done for image 16800\n",
      "Done for image 16900\n",
      "Done for image 17000\n",
      "Done for image 17100\n",
      "Done for image 17200\n",
      "Done for image 17300\n",
      "Done for image 17400\n",
      "Done for image 17500\n",
      "Done for image 17600\n",
      "Done for image 17700\n",
      "Done for image 17800\n",
      "Done for image 17900\n",
      "Done for image 18000\n",
      "Done for image 18100\n",
      "Done for image 18200\n",
      "Done for image 18300\n",
      "Done for image 18400\n",
      "Done for image 18500\n",
      "Done for train.\n",
      "Started for test...\n",
      "Done for image 0\n",
      "Done for image 100\n",
      "Done for image 200\n",
      "Done for image 300\n",
      "Done for image 400\n",
      "Done for image 500\n",
      "Done for image 600\n",
      "Done for image 700\n",
      "Done for image 800\n",
      "Done for image 900\n",
      "Done for image 1000\n",
      "Done for image 1100\n",
      "Done for image 1200\n",
      "Done for image 1300\n",
      "Done for image 1400\n",
      "Done for image 1500\n",
      "Done for image 1600\n",
      "Done for image 1700\n",
      "Done for image 1800\n",
      "Done for image 1900\n",
      "Done for image 2000\n",
      "Done for image 2100\n",
      "Done for image 2200\n",
      "Done for image 2300\n",
      "Done for image 2400\n",
      "Done for image 2500\n",
      "Done for image 2600\n",
      "Done for image 2700\n",
      "Done for image 2800\n",
      "Done for image 2900\n",
      "Done for image 3000\n",
      "Done for image 3100\n",
      "Done for image 3200\n",
      "Done for test\n"
     ]
    }
   ],
   "source": [
    "def semantic_extractor(sample_count, data, model):\n",
    "    semantic_features = np.zeros(shape=(sample_count, 151))\n",
    "    img_num = 0\n",
    "    \n",
    "    while img_num < sample_count:\n",
    "        img = data[img_num]*255\n",
    "        img = img.astype(np.uint8)\n",
    "        pil_img = Image.fromarray(img)\n",
    "        \n",
    "        resized_im, seg_map = model.run(pil_img)\n",
    "        for i in range(len(seg_map)):\n",
    "            for j in range(len(seg_map[i])):\n",
    "                semantic_features[img_num][seg_map[i][j]] += 1\n",
    "        if img_num % 100 == 0:\n",
    "            print(f'Done for image {img_num}')\n",
    "        img_num += 1\n",
    "        \n",
    "    return semantic_features\n",
    "\n",
    "print('Started for train...')\n",
    "train_semanticfeatures = semantic_extractor(18559, train_data, MODEL)\n",
    "print('Done for train.')\n",
    "print('Started for test...')\n",
    "test_semanticfeatures = semantic_extractor(3277, test_data, MODEL)\n",
    "print('Done for test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "735443b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(r'FeatureFiles/train_semanticfeatures.npy', train_semanticfeatures)\n",
    "np.save(r'FeatureFiles/test_semanticfeatures.npy', test_semanticfeatures)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
